{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"text generator.ipynb","provenance":[],"mount_file_id":"1JeWPyhrw9FvKcJWnbqFgRO-yUkgchoYH","authorship_tag":"ABX9TyOh8xRcTKiGXrDKBFfhRnL0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"7HTm0UZy79oY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658937047080,"user_tz":-330,"elapsed":1432167,"user":{"displayName":"PRASHANTH V 19BCB0066","userId":"02010670653683467041"}},"outputId":"a190a851-0feb-4ff1-9d40-8772ddc9deee"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n","84131840/84125825 [==============================] - 2s 0us/step\n","84140032/84125825 [==============================] - 2s 0us/step\n","50000 files\n","Epoch 1/20\n","generated text:\n","this movie is so it has a very good and very funny . it 's just how many things are the story . i don 't know that this movie is not a lot of the plot . but it wasn 't bad .\n","\n","391/391 - 71s - loss: 5.5601 - dense_2_loss: 5.5601 - 71s/epoch - 182ms/step\n","Epoch 2/20\n","generated text:\n","this movie is so stupid i don 't know where it 's not a comedy , but i have just wanted to see it and have a chance to watch it . . if you like the [UNK] and you have to see it\n","\n","391/391 - 68s - loss: 4.7113 - dense_2_loss: 4.7113 - 68s/epoch - 173ms/step\n","Epoch 3/20\n","generated text:\n","this movie is a great movie , and it shows it to the same as one . it is a nice combination of a great action thriller . . . . . . . . . .and the only reason it 's so it\n","\n","391/391 - 68s - loss: 4.4669 - dense_2_loss: 4.4669 - 68s/epoch - 174ms/step\n","Epoch 4/20\n","generated text:\n","this movie is not one of my favorite movies about the year and i am really impressed this is by the way it is a classic film , which the best . i don 't even feel the way it 's because there is\n","\n","391/391 - 68s - loss: 4.3129 - dense_2_loss: 4.3129 - 68s/epoch - 174ms/step\n","Epoch 5/20\n","generated text:\n","this movie is about an african -american family in the past , who is the only one of their favorite actresses , and the film is the story of a family film . it takes the first off the first two minutes of the\n","\n","391/391 - 68s - loss: 4.1946 - dense_2_loss: 4.1946 - 68s/epoch - 174ms/step\n","Epoch 6/20\n","generated text:\n","this movie is the only good time , i love it . it is very sad . the movie does not know why i was the first time . i have been so much more money than the film -makers who are in the\n","\n","391/391 - 68s - loss: 4.0966 - dense_2_loss: 4.0966 - 68s/epoch - 174ms/step\n","Epoch 7/20\n","generated text:\n","this movie is so bad i can say this is the worst movie ever seen . a lot of people , it 's not even more of the most stupid movies i 've seen in the theater and it is just bad . i\n","\n","391/391 - 68s - loss: 4.0124 - dense_2_loss: 4.0124 - 68s/epoch - 174ms/step\n","Epoch 8/20\n","generated text:\n","this movie is a disgrace . it is one of my favorites . the story is not much to do the fact that i can 't wait for the next to it ! ! ! ! ! [UNK] ! ! ! ! ! !\n","\n","391/391 - 68s - loss: 3.9390 - dense_2_loss: 3.9390 - 68s/epoch - 174ms/step\n","Epoch 9/20\n","generated text:\n","this movie is not just as bad as i am a big fan . however , it was terrible and it just didn 't really get me wrong . my god , i 'm not the movie to think of [UNK] , but i\n","\n","391/391 - 68s - loss: 3.8736 - dense_2_loss: 3.8736 - 68s/epoch - 174ms/step\n","Epoch 10/20\n","generated text:\n","this movie is an insult to all the characters , but the script is poor and poor , the entire film was badly acted . there is a few plot -holes , it 's not even as bad as they are as bad as\n","\n","391/391 - 68s - loss: 3.8137 - dense_2_loss: 3.8137 - 68s/epoch - 174ms/step\n","Epoch 11/20\n","generated text:\n","this movie is really bad . . . . . . . . . . . . . . i thought it wasn 't . i think it was the only movie about it and that it wasn 't until i saw it .\n","\n","391/391 - 68s - loss: 3.7610 - dense_2_loss: 3.7610 - 68s/epoch - 174ms/step\n","Epoch 12/20\n","generated text:\n","this movie is really bad and not really expecting anything . if you like this movie could have been better . the plot is about something like this movie , and the actors are all fantastic . the only thing the characters in the\n","\n","391/391 - 68s - loss: 3.7126 - dense_2_loss: 3.7126 - 68s/epoch - 174ms/step\n","Epoch 13/20\n","generated text:\n","this movie is so [UNK] it 's not a classic ! i can 't even tell you that it 's actually pretty darn funny , but i love it . i 'm still trying to figure out what i have to say is the\n","\n","391/391 - 68s - loss: 3.6676 - dense_2_loss: 3.6676 - 68s/epoch - 174ms/step\n","Epoch 14/20\n","generated text:\n","this movie is not about a film that will be a great movie , but it is just a bunch of people that have just been [UNK] \" . i am very surprised how to give it a try . i am not sure\n","\n","391/391 - 68s - loss: 3.6269 - dense_2_loss: 3.6269 - 68s/epoch - 174ms/step\n","Epoch 15/20\n","generated text:\n","this movie is not about a young gay soccer game ; but what a disappointment ! in its way it is so bad , but it fails miserably . . it had a terrible acting and the characters , it is not funny .\n","\n","391/391 - 68s - loss: 3.5889 - dense_2_loss: 3.5889 - 68s/epoch - 174ms/step\n","Epoch 16/20\n","generated text:\n","this movie is really about an [UNK] in [UNK] and the end it is so hard it [UNK] of the world that it has been repeated beatings at the hands of his life that was not very [UNK] the movie [UNK] [UNK] to make\n","\n","391/391 - 68s - loss: 3.5533 - dense_2_loss: 3.5533 - 68s/epoch - 174ms/step\n","Epoch 17/20\n","generated text:\n","this movie is a disgrace to the league with the likes of the movie . . it 's just the acting , and the script was terrible . the acting , the special effects are ok but the story was pretty awful . the\n","\n","391/391 - 68s - loss: 3.5217 - dense_2_loss: 3.5217 - 68s/epoch - 174ms/step\n","Epoch 18/20\n","generated text:\n","this movie is really bad . it 's a fun and entertaining film . it is a good movie . it 's a very good way . it 's a simple story of a young man who is a boy living in the lives\n","\n","391/391 - 68s - loss: 3.4916 - dense_2_loss: 3.4916 - 68s/epoch - 174ms/step\n","Epoch 19/20\n","generated text:\n","this movie is an awesome movie ! ! ! ! ! i have to say that this is the first film that 's the first one , which is pretty bad , is not a bad movie , but it 's not the worst\n","\n","391/391 - 68s - loss: 3.4636 - dense_2_loss: 3.4636 - 68s/epoch - 174ms/step\n","Epoch 20/20\n","generated text:\n","this movie is a must see . a lot of the plot revolves around a group of soldiers who discover a japanese soldiers under attack force of their monsters or are they [UNK] . the film is not bad . i think that it\n","\n","391/391 - 68s - loss: 3.4375 - dense_2_loss: 3.4375 - 68s/epoch - 174ms/step\n"]}],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","#from tensorflow.keras.layers import TextVectorization\n","import numpy as np\n","import os\n","import re\n","import string\n","import random\n","from keras.layers import TextVectorization\n","import pathlib\n","def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n","\n","    i = tf.range(n_dest)[:, None]\n","    j = tf.range(n_src)\n","    m = i >= j - n_src + n_dest\n","    mask = tf.cast(m, dtype)\n","    mask = tf.reshape(mask, [1, n_dest, n_src])\n","    mult = tf.concat(\n","        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n","    )\n","    return tf.tile(mask, mult)\n","\n","#creating a tranformer model using subclassing\n","\n","class TransformerBlock(layers.Layer):\n","    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n","        super(TransformerBlock, self).__init__()\n","        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n","        self.ffn = keras.Sequential(\n","            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n","        )\n","        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n","        self.dropout1 = layers.Dropout(rate)\n","        self.dropout2 = layers.Dropout(rate)\n","\n","    def call(self, inputs):\n","        input_shape = tf.shape(inputs)\n","        batch_size = input_shape[0]\n","        seq_len = input_shape[1]\n","        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n","        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n","        attention_output = self.dropout1(attention_output)\n","        out1 = self.layernorm1(inputs + attention_output)\n","        ffn_output = self.ffn(out1)\n","        ffn_output = self.dropout2(ffn_output)\n","        return self.layernorm2(out1 + ffn_output)\n","\n","class TokenAndPositionEmbedding(layers.Layer):\n","    def __init__(self, maxlen, vocab_size, embed_dim):\n","        super(TokenAndPositionEmbedding, self).__init__()\n","        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n","        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n","\n","    def call(self, x):\n","        maxlen = tf.shape(x)[-1]\n","        positions = tf.range(start=0, limit=maxlen, delta=1)\n","        positions = self.pos_emb(positions)\n","        x = self.token_emb(x)\n","        return x + positions\n","\n","#creating the model\n","\n","vocab_size = 20000  # Only consider the top 20k words\n","maxlen = 80  # Max sequence size\n","embed_dim = 256  # Embedding size for each token\n","num_heads = 2  # Number of attention heads\n","feed_forward_dim = 256  # Hidden layer size in feed forward network inside transformer\n","\n","\n","def create_model():\n","    inputs = layers.Input(shape=(maxlen,), dtype=tf.int32)\n","    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n","    x = embedding_layer(inputs)\n","    transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n","    x = transformer_block(x)\n","    outputs = layers.Dense(vocab_size)(x)\n","    model = keras.Model(inputs=inputs, outputs=[outputs, x])\n","    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","    model.compile(\n","        \"adam\", loss=[loss_fn, None],\n","    )  # No loss and optimization based on word embeddings from transformer block\n","    return model\n","\n","batch_size = 128\n","\n","dataset = tf.keras.utils.get_file(\n","    fname=\"aclImdb.tar.gz\", \n","    origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n","    extract=True,\n","    untar=True \n",")\n","\n","\n","# The dataset contains each review in a separate text file\n","# The text files are present in four different folders\n","# Create a list all files\n","filenames = []\n","directories = [\n","    dataset + \"/train/pos\",\n","    dataset + \"/train/neg\",\n","    dataset + \"/test/pos\",\n","    dataset + \"/test/neg\",\n","]\n","\n","\n","for dir in directories:\n","    for f in os.listdir(dir):\n","        filenames.append(os.path.join(dir, f))\n","\n","print(f\"{len(filenames)} files\")\n","\n","# Create a dataset from text files\n","random.shuffle(filenames)\n","text_ds = tf.data.TextLineDataset(filenames)\n","text_ds = text_ds.shuffle(buffer_size=256)\n","text_ds = text_ds.batch(batch_size)\n","\n","\n","def custom_standardization(input_string):\n","    \"\"\" Remove html line-break tags and handle punctuation \"\"\"\n","    lowercased = tf.strings.lower(input_string)\n","    stripped_html = tf.strings.regex_replace(lowercased, \"<br />\", \" \")\n","    return tf.strings.regex_replace(stripped_html, f\"([{string.punctuation}])\", r\" \\1\")\n","\n","\n","# Create a vectorization layer and adapt it to the text\n","vectorize_layer = TextVectorization(\n","    standardize=custom_standardization,\n","    max_tokens=vocab_size - 1,\n","    output_mode=\"int\",\n","    output_sequence_length=maxlen + 1,\n",")\n","vectorize_layer.adapt(text_ds)\n","vocab = vectorize_layer.get_vocabulary()  # To get words back from token indices\n","\n","\n","def prepare_lm_inputs_labels(text):\n","    \"\"\"\n","    Shift word sequences by 1 position so that the target for position (i) is\n","    word at position (i+1). The model will use all words up till position (i)\n","    to predict the next word.\n","    \"\"\"\n","    text = tf.expand_dims(text, -1)\n","    tokenized_sentences = vectorize_layer(text)\n","    x = tokenized_sentences[:, :-1]\n","    y = tokenized_sentences[:, 1:]\n","    return x, y\n","\n","\n","text_ds = text_ds.map(prepare_lm_inputs_labels)\n","text_ds = text_ds.prefetch(tf.data.AUTOTUNE)\n","\n","class TextGenerator(keras.callbacks.Callback):\n","    \"\"\"A callback to generate text from a trained model.\n","    1. Feed some starting prompt to the model\n","    2. Predict probabilities for the next token\n","    3. Sample the next token and add it to the next input\n","\n","    Arguments:\n","        max_tokens: Integer, the number of tokens to be generated after prompt.\n","        start_tokens: List of integers, the token indices for the starting prompt.\n","        index_to_word: List of strings, obtained from the TextVectorization layer.\n","        top_k: Integer, sample from the `top_k` token predictions.\n","        print_every: Integer, print after this many epochs.\n","    \"\"\"\n","\n","    def __init__(\n","        self, max_tokens, start_tokens, index_to_word, top_k=10, print_every=1\n","    ):\n","        self.max_tokens = max_tokens\n","        self.start_tokens = start_tokens\n","        self.index_to_word = index_to_word\n","        self.print_every = print_every\n","        self.k = top_k\n","\n","    def sample_from(self, logits):\n","        logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n","        indices = np.asarray(indices).astype(\"int32\")\n","        preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n","        preds = np.asarray(preds).astype(\"float32\")\n","        return np.random.choice(indices, p=preds)\n","\n","    def detokenize(self, number):\n","        return self.index_to_word[number]\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        start_tokens = [_ for _ in self.start_tokens]\n","        if (epoch + 1) % self.print_every != 0:\n","            return\n","        num_tokens_generated = 0\n","        tokens_generated = []\n","        while num_tokens_generated <= self.max_tokens:\n","            pad_len = maxlen - len(start_tokens)\n","            sample_index = len(start_tokens) - 1\n","            if pad_len < 0:\n","                x = start_tokens[:maxlen]\n","                sample_index = maxlen - 1\n","            elif pad_len > 0:\n","                x = start_tokens + [0] * pad_len\n","            else:\n","                x = start_tokens\n","            x = np.array([x])\n","            y, _ = self.model.predict(x)\n","            sample_token = self.sample_from(y[0][sample_index])\n","            tokens_generated.append(sample_token)\n","            start_tokens.append(sample_token)\n","            num_tokens_generated = len(tokens_generated)\n","        txt = \" \".join(\n","            [self.detokenize(_) for _ in self.start_tokens + tokens_generated]\n","        )\n","        print(f\"generated text:\\n{txt}\\n\")\n","\n","\n","# Tokenize starting prompt\n","word_to_index = {}\n","for index, word in enumerate(vocab):\n","    word_to_index[word] = index\n","\n","start_prompt = \"this movie is\"\n","start_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\n","num_tokens_generated = 40\n","text_gen_callback = TextGenerator(num_tokens_generated, start_tokens, vocab)\n","\n","model = create_model()\n","\n","model.fit(text_ds, verbose=2, epochs=20, callbacks=[text_gen_callback])\n","\n","model.save_weights(\"weightsM.h5\")\n"]}]}